from benchmark import Unit
from memory import memset_zero, stack_allocation
from random import rand
from algorithm import vectorize, parallelize, vectorize_unroll
from algorithm import Static2DTileUnitFunc as Tile2DFunc
from python import Python
from tensor import Tensor
from utils.index import Index
from memory.buffer import NDBuffer

alias M = 512
alias N = 512
alias K = 4096
alias type = DType.float32

struct Matrix:
    var data: DTypePointer[type]
    var rows: Int
    var cols: Int

    # Initialize zeroeing all values
    fn __init__(inout self, rows: Int, cols: Int):
        self.data = DTypePointer[type].alloc(rows * cols)
        memset_zero(self.data, rows * cols)
        self.rows = rows
        self.cols = cols

    # Initialize taking a pointer, don't set any elements
    fn __init__(
        inout self, rows: Int, cols: Int, data: DTypePointer[type]
    ):
        self.data = data
        self.rows = rows
        self.cols = cols

    ## Initialize with random values
    @staticmethod
    fn rand(rows: Int, cols: Int) -> Self:
        let data = DTypePointer[type].alloc(rows * cols)
        rand(data, rows * cols)
        return Self(rows, cols, data)

    fn __getitem__(self, y: Int, x: Int) -> SIMD[type, 1]:
        return self.load[1](y, x)

    fn __setitem__(inout self, y: Int, x: Int, val: SIMD[type, 1]):
        self.store[1](y, x, val)

    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[type, nelts]:
        return self.data.simd_load[nelts](y * self.cols + x)

    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[type, nelts]):
        return self.data.simd_store[nelts](y * self.cols + x, val)


def run_matmul_python() -> Float64:
    Python.add_to_path(".")
    let pymatmul: PythonObject = Python.import_module("pymatmul")
    let py = Python.import_module("builtins")

    let gflops = pymatmul.benchmark_matmul_python(128, 128, 128).to_float64()
    py.print(py.str("{:<13}{:>8.3f} GFLOPS").format("Python:", gflops))

    return gflops


def run_matmul_numpy() -> Float64:
    let pymatmul: PythonObject = Python.import_module("pymatmul")
    let py = Python.import_module("builtins")

    let gflops = pymatmul.benchmark_matmul_numpy(M, N, K).to_float64()
    py.print(py.str("{:<13}{:>8.3f} GFLOPS").format("Numpy:", gflops))

    return gflops


fn matmul_naive(inout C: Matrix, A: Matrix, B: Matrix):
    for m in range(C.rows):
        for k in range(A.cols):
            for n in range(C.cols):
                C[m, n] += A[m, k] * B[k, n]


# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.
alias nelts = simdwidthof[type]()  # The SIMD vector width.


# Using stdlib vectorize function
fn matmul_vectorized(inout C: Matrix, A: Matrix, B: Matrix):
    for m in range(C.rows):
        for k in range(A.cols):

            @parameter
            fn dot[nelts: Int](n: Int):
                C.store[nelts](
                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)
                )

            vectorize[nelts, dot](C.cols)


# Parallelize the code by using the builtin parallelize function
fn matmul_parallelized(inout C: Matrix, A: Matrix, B: Matrix):
    @parameter
    fn calc_row(m: Int):
        for k in range(A.cols):

            @parameter
            fn dot[nelts: Int](n: Int):
                C.store[nelts](
                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)
                )

            vectorize[nelts, dot](C.cols)

    parallelize[calc_row](C.rows, C.rows)


# Perform 2D tiling on the iteration space defined by end_x and end_y.
fn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):
    # Note: this assumes that ends are multiples of the tiles.
    for y in range(0, end_y, tile_y):
        for x in range(0, end_x, tile_x):
            tiled_fn[tile_x, tile_y](x, y)


# Use the above tile function to perform tiled matmul.
fn matmul_tiled(inout C: Matrix, A: Matrix, B: Matrix):
    @parameter
    fn calc_row(m: Int):
        @parameter
        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):
            for k in range(y, y + tile_y):

                @parameter
                fn dot[
                    nelts: Int,
                ](n: Int):
                    C.store[nelts](
                        m,
                        n + x,
                        C.load[nelts](m, n + x)
                        + A[m, k] * B.load[nelts](k, n + x),
                    )

                vectorize[nelts, dot](tile_x)

        # We hardcode the tile factor to be 4.
        alias tile_size = 4
        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)

    parallelize[calc_row](C.rows, C.rows)


# Unroll the vectorized loop by a constant factor.
# from Functional import vectorize_unroll
fn matmul_unrolled(inout C: Matrix, A: Matrix, B: Matrix):
    alias tile_size = 4

    @parameter
    fn calc_row(m: Int):
        @parameter
        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):
            for k in range(y, y + tile_y):

                @parameter
                fn dot[
                    nelts: Int,
                ](n: Int):
                    C.store[nelts](
                        m,
                        n + x,
                        C.load[nelts](m, n + x)
                        + A[m, k] * B.load[nelts](k, n + x),
                    )

                # Vectorize by nelts and unroll by tile_x/nelts
                # Here unroll factor is 4
                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)

        alias tile_size = 4
        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)

    parallelize[calc_row](C.rows, C.rows)


# Perform 2D tiling on the iteration space defined by end_x and end_y, parallelizing over y.
fn tile_parallel[
    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int
](end_x: Int, end_y: Int):
    # Note: this assumes that ends are multiples of the tiles.
    @parameter
    fn row(yo: Int):
        let y = tile_y * yo
        for x in range(0, end_x, tile_x):
            tiled_fn[tile_x, tile_y](x, y)

    parallelize[row](end_y // tile_y, M)


# Use stack allocation for tiles to accumulate values efficiently,
# avoiding repeated reads and writes to memory. Also reorder the loops
# and do not fully unroll the loop over the reduction dimension.
fn matmul_accumulated(inout C: Matrix, A: Matrix, B: Matrix):
    alias tile_k = 8
    alias tile_k_unroll = 8
    alias tile_i = 32
    alias tile_j = nelts * 4

    @parameter
    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):
        # Allocate the tile of accumulators on the stack.
        var accumulators = Matrix(
            tile_i, tile_j, stack_allocation[tile_i * tile_j, type]()
        )

        for ko in range(0, A.cols, tile_k * tile_k_unroll):
            for _ in range(tile_i):
                for i in range(tile_k):

                    @unroll
                    for k in range(tile_k_unroll):

                        @parameter
                        fn calc_tile_cols[nelts: Int](j: Int):
                            accumulators.store[nelts](
                                i,
                                j,
                                accumulators.load[nelts](i, j)
                                + A[io + i, ko + k]
                                * B.load[nelts](ko + k, jo + j),
                            )

                        vectorize_unroll[
                            nelts, tile_j // nelts, calc_tile_cols
                        ](tile_j)

        # Copy the local tile to the output
        for i in range(tile_i):
            for j in range(tile_j):
                C[io + i, jo + j] = accumulators[i, j]

    tile_parallel[calc_tile, tile_j, tile_i](C.cols, C.rows)


@always_inline
fn bench[
    func: fn (inout Matrix, Matrix, Matrix) -> None, name: StringLiteral
](base_gflops: Float64, numpy_gflops: Float64) raises:
    var A = Matrix.rand(M, K)
    var B = Matrix.rand(K, N)
    var C = Matrix(M, N)

    @always_inline
    @parameter
    fn test_fn():
        _ = func(C, A, B)

    let secs = benchmark.run[test_fn](max_runtime_secs=0.5).mean()
    # Prevent the matrices from being freed before the benchmark run
    A.data.free()
    B.data.free()
    C.data.free()
    let gflops = ((2 * M * N * K) / secs) / 1e9
    let speedup: Float64 = gflops / base_gflops
    let numpy_speedup: Float64 = gflops / numpy_gflops

    let py = Python.import_module("builtins")
    _ = py.print(
        py.str("{:<13}{:>8.3f} GFLOPS {:>9.2f}x Python {:>5.2f}x Numpy").format(
            name, gflops, speedup, numpy_speedup
        )
    )


@always_inline
fn test[
    func: fn (inout Matrix, Matrix, Matrix) -> None
](A: Matrix, B: Matrix) raises -> SIMD[type, 1]:
    var C = Matrix(M, N)
    _ = func(C, A, B)
    var result = SIMD[type, 1]()
    for i in range(C.rows):
        for j in range(C.cols):
            result += C[i, j]
    return result


fn test_all() raises:
    constrained[M == N, "M and N must be equal for matrix multiplication"]()

    let A = Matrix.rand(M, K)
    let B = Matrix.rand(K, N)

    let result = test[matmul_naive](A, B)

    if test[matmul_vectorized](A, B) != result:
        raise Error("Vectorize output does not match")
    if test[matmul_parallelized](A, B) != result:
        raise Error("Parallelize output incorrect")
    if test[matmul_tiled](A, B) != result:
        raise Error("Tiled output incorrect")
    if test[matmul_unrolled](A, B) != result:
        raise Error("Unroll output incorrect")
    if test[matmul_accumulated](A, B) != result:
        raise Error("Loop reorder output incorrect")

    A.data.free()
    B.data.free()


fn main() raises:
    # Uncomment below to test correctness of Matmuls
    # test_all()
    print("CPU Results\n")
    let python_gflops = run_matmul_python()
    let numpy_gflops = run_matmul_numpy()

    bench[matmul_naive, "Naive:"](python_gflops, numpy_gflops)
    bench[matmul_vectorized, "Vectorized: "](python_gflops, numpy_gflops)
    bench[matmul_parallelized, "Parallelized:"](python_gflops, numpy_gflops)
    bench[matmul_tiled, "Tiled:"](python_gflops, numpy_gflops)
    bench[matmul_unrolled, "Unrolled:"](python_gflops, numpy_gflops)
    bench[matmul_accumulated, "Accumulated:"](python_gflops, numpy_gflops)

# This sample implements various memset algorithms and optimizations

from autotune import autotune_fork
from math import min, max
from time import time_function
from memory import memset as stdlib_memset
from benchmark import keep

alias type = UInt8
alias ptr_type = DTypePointer[DType.uint8]
alias fn_type = fn (ptr_type, type, Int) -> None


fn measure_time(func: fn_type, size: Int, iters: Int, samples: Int) -> Int:
    alias alloc_size = 1024 * 1024
    let ptr = ptr_type.alloc(alloc_size)

    var best = -1
    for sample in range(samples):

        @parameter
        fn runner():
            for iter in range(iters):
                # Offset pointer to shake up cache a bit
                let offset_ptr = ptr.offset((iter * 128) & 1024)

                # memset, change the value we're filling with
                let v = type(iter&255)

                # Actually call the memset function
                func(offset_ptr, v.value, size)

                # Avoid compiler optimizing things away
                keep(v)
                keep(size)
                keep(offset_ptr)

        let ns = time_function[runner]()
        if best < 0 or ns < best:
            best = ns

    ptr.free()
    return best


alias MULT = 2_000


fn visualize_result(size: Int, result: Int):
    print_no_newline("Size: ")
    if size < 10:
        print_no_newline(" ")
    print_no_newline(size, "  |")
    for _ in range(result // MULT):
        print_no_newline("*")
    print()


fn benchmark(func: fn_type, title: StringRef):
    print("\n=====================")
    print(title)
    print("---------------------\n")

    alias benchmark_iterations = 30 * MULT
    alias warmup_samples = 10
    alias benchmark_samples = 1000

    # Warmup
    for size in range(35):
        _ = measure_time(func, size, benchmark_iterations, warmup_samples)

    # Actual run
    for size in range(35):
        let result = measure_time(
            func, size, benchmark_iterations, benchmark_samples
        )

        visualize_result(size, result)


@always_inline
fn overlapped_store[width: Int](ptr: ptr_type, value: type, count: Int):
    let v = SIMD[DType.uint8, width].splat(value)
    ptr.simd_store[width](v)
    ptr.simd_store[width](count - width, v)


fn memset_manual(ptr: ptr_type, value: type, count: Int):
    if count < 32:
        if count < 5:
            if count == 0:
                return
            # 0 < count <= 4
            ptr.store(0, value)
            ptr.store(count - 1, value)
            if count <= 2:
                return
            ptr.store(1, value)
            ptr.store(count - 2, value)
            return

        if count <= 16:
            if count >= 8:
                # 8 <= count < 16
                overlapped_store[8](ptr, value, count)
                return
            # 4 < count < 8
            overlapped_store[4](ptr, value, count)
            return

        # 16 <= count < 32
        overlapped_store[16](ptr, value, count)
    else:
        # 32 < count
        memset_system(ptr, value, count)


fn memset_system(ptr: ptr_type, value: type, count: Int):
    stdlib_memset(ptr, value.value, count)


fn memset_manual_2(ptr: ptr_type, value: type, count: Int):
    if count < 32:
        if count >= 16:
            # 16 <= count < 32
            overlapped_store[16](ptr, value, count)
            return

        if count < 5:
            if count == 0:
                return
            # 0 < count <= 4
            ptr.store(0, value)
            ptr.store(count - 1, value)
            if count <= 2:
                return
            ptr.store(1, value)
            ptr.store(count - 2, value)
            return

        if count >= 8:
            # 8 <= count < 16
            overlapped_store[8](ptr, value, count)
            return
        # 4 < count < 8
        overlapped_store[4](ptr, value, count)

    else:
        # 32 < count
        memset_system(ptr, value, count)


@adaptive
@always_inline
fn memset_impl_layer[
    lower: Int, upper: Int
](ptr: ptr_type, value: type, count: Int):
    @parameter
    if lower == -100 and upper == 0:
        pass
    elif lower == 0 and upper == 4:
        ptr.store(0, value)
        ptr.store(count - 1, value)
        if count <= 2:
            return
        ptr.store(1, value)
        ptr.store(count - 2, value)
    elif lower == 4 and upper == 8:
        overlapped_store[4](ptr, value, count)
    elif lower == 8 and upper == 16:
        overlapped_store[8](ptr, value, count)
    elif lower == 16 and upper == 32:
        overlapped_store[16](ptr, value, count)
    elif lower == 32 and upper == 100:
        memset_system(ptr, value, count)
    else:
        constrained[False]()


@adaptive
@always_inline
fn memset_impl_layer[
    lower: Int, upper: Int
](ptr: ptr_type, value: type, count: Int):
    alias cur: Int
    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()

    constrained[cur > lower]()
    constrained[cur < upper]()

    if count > cur:
        memset_impl_layer[max(cur, lower), upper](ptr, value, count)
    else:
        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)


@adaptive
@always_inline
fn memset_impl_layer[
    lower: Int, upper: Int
](ptr: ptr_type, value: type, count: Int):
    alias cur: Int
    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()

    constrained[cur > lower]()
    constrained[cur < upper]()

    if count <= cur:
        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)
    else:
        memset_impl_layer[max(cur, lower), upper](ptr, value, count)


fn memset_evaluator(funcs: Pointer[fn_type], size: Int) -> Int:
    # This size is picked at random, in real code we could use a real size
    # distribution here.
    let size_to_optimize_for = 17
    print("Optimizing for size: ", size_to_optimize_for)

    var best_idx: Int = -1
    var best_time: Int = -1

    alias eval_iterations = MULT
    alias eval_samples = 500

    # Find the function that's the fastest on the size we're optimizing for
    for f_idx in range(size):
        let func = funcs.load(f_idx)
        let cur_time = measure_time(
            func, size_to_optimize_for, eval_iterations, eval_samples
        )
        if best_idx < 0:
            best_idx = f_idx
            best_time = cur_time
        if best_time > cur_time:
            best_idx = f_idx
            best_time = cur_time

    return best_idx


fn main():
    # CHECK: Manual memset
    benchmark(memset_manual, "Manual memset")
    # CHECK: System memset
    benchmark(memset_system, "System memset")
    # CHECK: Manual memset v2
    benchmark(memset_manual_2, "Manual memset v2")
    # CHECK: Mojo system memset
    benchmark(memset_system, "Mojo system memset")
    # CHECK: Mojo manual memset
    benchmark(memset_manual, "Mojo manual memset")
    # CHECK: Mojo manual memset v2
    benchmark(memset_manual_2, "Mojo manual memset v2")
    # CHECK: Mojo system memset
    benchmark(memset_system, "Mojo system memset")